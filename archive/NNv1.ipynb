{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2 as cv\n",
    "import os\n",
    "from feature_extraction import *\n",
    "\n",
    "IMG_WIDTH = 64\n",
    "IMG_HEIGHT = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eqn:\n",
    "    def __init__(self, y):\n",
    "        self.y = y\n",
    "    \n",
    "    def eval(self, x):\n",
    "        return (self.y[0] ** x) + (self.y[1] * x) + self.y[2]\n",
    "\n",
    "def flatten(x):\n",
    "    final = []\n",
    "    for pos1 in range(len(x)):\n",
    "        for pos2 in range(len(x[0])):\n",
    "            for pos3 in range(len(x[0][0])):\n",
    "                final.append(x[pos1][pos2][pos3])\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for path in os.listdir(os.getcwd() + \"\\\\Sidewalks\"):\n",
    "    img = cv.imread(os.getcwd() + \"\\\\Sidewalks\\\\\" + path)\n",
    "    features.append(flatten([edge_detection(img, IMG_WIDTH, IMG_HEIGHT), hue_detection(img, IMG_WIDTH, IMG_HEIGHT)]))\n",
    "\n",
    "train = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(features, lines):\n",
    "    losses = []\n",
    "    for pos in range(0, len(features)):\n",
    "        x = features.numpy()[pos]\n",
    "        y = lines.numpy()[pos]\n",
    "        \n",
    "        SD = 10\n",
    "        HEIGHT = 40\n",
    "        normal = lambda SD, HEIGHT, M, x: (1/(SD*math.sqrt(2*math.pi))) * ((math.e ** ((-1/2) * (((x-M) / SD) ** 2)))) * HEIGHT\n",
    "\n",
    "        line = eqn(y)\n",
    "        mid = int(len(x) / 2)\n",
    "\n",
    "        max = 0\n",
    "        loss1 = 0\n",
    "        loss2 = 0\n",
    "        for x_pos in range(0, IMG_WIDTH):\n",
    "            y_line_pos = line.eval(x_pos)\n",
    "            for y_pos in range(0, IMG_HEIGHT):\n",
    "                val = normal(SD, HEIGHT, y_line_pos, y_pos)\n",
    "                pos = int((IMG_WIDTH * y_pos) + x_pos)\n",
    "                loss1 += x[pos] * val / 255\n",
    "                loss2 += x[pos + mid] * val / 255\n",
    "                max += val\n",
    "        if max != 0:\n",
    "            losses.append(min(((max - loss1) / max), ((max - loss2) / max)))\n",
    "        else:\n",
    "            losses.append(1)\n",
    "    return tf.convert_to_tensor(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  return tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(4096, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, kernel_initializer='RandomNormal', activation='relu'),\n",
    "    tf.keras.layers.Dense(3)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.compile(loss=custom_loss, optimizer='adam', metrics=['accuracy'], run_eagerly=True)\n",
    "model.fit(x=train, \n",
    "          y=train, \n",
    "          epochs=20, \n",
    "          validation_data=(train, train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
